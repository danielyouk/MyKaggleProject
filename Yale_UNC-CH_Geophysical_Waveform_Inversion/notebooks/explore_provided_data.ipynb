{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Exploring Provided Waveform Inversion Data\n",
    "\n",
    "This notebook loads and visualizes the seismic data (`.npy` files) and velocity model data (`.npy` files) provided in the `waveform_inversion_data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_dir = os.path.join(\"..\", \"waveform_inversion_data\")\n",
    "seismic_file_train = os.path.join(data_dir, \"seis2_1_0.npy\")\n",
    "velocity_file_train = os.path.join(data_dir, \"vel2_1_0.npy\")\n",
    "seismic_file_test = os.path.join(data_dir, \"000039dca2.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seismic_data_train = np.load(seismic_file_train, mmap_mode='r')\n",
    "velocity_data_train = np.load(velocity_file_train, mmap_mode='r') \n",
    "seismic_data_test = np.load(seismic_file_test, mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1760.     1760.     1760.     ... 1760.     1760.     1760.    ]\n",
      "   [1760.     1760.     1760.     ... 1760.     1760.     1760.    ]\n",
      "   [1760.     1760.     1760.     ... 1760.     1760.     1760.    ]\n",
      "   ...\n",
      "   [3455.     3455.     3455.     ... 3455.     3455.     3455.    ]\n",
      "   [3455.     3455.     3455.     ... 3455.     3455.     3455.    ]\n",
      "   [3455.     3455.     3455.     ... 3455.     3455.     3455.    ]]]\n",
      "\n",
      "\n",
      " [[[2988.     2988.     2988.     ... 2988.     2988.     2988.    ]\n",
      "   [2988.     2988.     2988.     ... 2988.     2988.     2988.    ]\n",
      "   [2988.     2988.     2988.     ... 2988.     2988.     2988.    ]\n",
      "   ...\n",
      "   [4404.     4404.     4404.     ... 4404.     4404.     4404.    ]\n",
      "   [4404.     4404.     4404.     ... 4404.     4404.     4404.    ]\n",
      "   [4404.     4404.     4404.     ... 4404.     4404.     4404.    ]]]\n",
      "\n",
      "\n",
      " [[[1676.     1676.     1676.     ... 1676.     1676.     1676.    ]\n",
      "   [1676.     1676.     1676.     ... 1676.     1676.     1676.    ]\n",
      "   [1676.     1676.     1676.     ... 1676.     1676.     1676.    ]\n",
      "   ...\n",
      "   [3375.     3375.     3375.     ... 3375.     3375.     3375.    ]\n",
      "   [3375.     3375.     3375.     ... 3375.     3375.     3375.    ]\n",
      "   [3375.     3375.     3375.     ... 3375.     3375.     3375.    ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[1628.     1628.     1628.     ... 1628.     1628.     1628.    ]\n",
      "   [1628.     1628.     1628.     ... 1628.     1628.     1628.    ]\n",
      "   [1628.     1628.     1628.     ... 1628.     1628.     1628.    ]\n",
      "   ...\n",
      "   [3927.     3927.     3927.     ... 3926.9438 3927.     3927.    ]\n",
      "   [3927.     3927.     3927.     ... 3927.     3927.     3927.    ]\n",
      "   [3927.     3927.     3927.     ... 3927.     3927.     3927.    ]]]\n",
      "\n",
      "\n",
      " [[[1600.     1600.     1600.     ... 1600.     1600.     1600.    ]\n",
      "   [1600.     1600.     1600.0002 ... 1600.     1600.     1600.    ]\n",
      "   [1600.     1600.0015 1600.0217 ... 1600.     1600.     1600.    ]\n",
      "   ...\n",
      "   [3221.     3221.     3221.     ... 3221.     3221.     3221.    ]\n",
      "   [3221.     3221.     3221.     ... 3221.     3221.     3221.    ]\n",
      "   [3221.     3221.     3221.     ... 3221.     3221.     3221.    ]]]\n",
      "\n",
      "\n",
      " [[[2470.     2470.     2470.     ... 2470.     2470.     2470.    ]\n",
      "   [2470.     2470.     2470.     ... 2470.     2470.     2470.    ]\n",
      "   [2470.     2470.     2470.     ... 2470.     2470.     2470.    ]\n",
      "   ...\n",
      "   [3071.     3071.     3071.     ... 3071.     3071.     3071.    ]\n",
      "   [3071.     3071.     3071.     ... 3071.     3071.     3071.    ]\n",
      "   [3071.     3071.     3071.     ... 3071.     3071.     3071.    ]]]]\n"
     ]
    }
   ],
   "source": [
    "print(velocity_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-4.3723936e-04  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [-1.4025457e-03 -1.5501239e-05  9.6809958e-07 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [-2.9656747e-03 -7.8596757e-05  4.3638470e-06 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  ...\n",
      "  [-2.1746773e-03 -2.2501172e-03 -2.3082583e-03 ...  4.8055989e-03\n",
      "    5.8574509e-03  6.0836617e-03]\n",
      "  [-2.1236797e-03 -2.2126497e-03 -2.2827697e-03 ...  5.3145797e-03\n",
      "    6.0396837e-03  5.9555816e-03]\n",
      "  [-2.0684560e-03 -2.1702910e-03 -2.2520840e-03 ...  5.7014483e-03\n",
      "    6.1006825e-03  5.7227770e-03]]\n",
      "\n",
      " [[ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  ...\n",
      "  [-2.2970506e-03 -2.3217648e-03 -2.3477431e-03 ... -6.2175114e-03\n",
      "   -7.0539224e-03 -8.0095967e-03]\n",
      "  [-2.2877140e-03 -2.3088739e-03 -2.3381393e-03 ... -6.5338514e-03\n",
      "   -7.4246740e-03 -8.4184501e-03]\n",
      "  [-2.2783997e-03 -2.2961725e-03 -2.3290662e-03 ... -6.8740137e-03\n",
      "   -7.8133270e-03 -8.8363411e-03]]\n",
      "\n",
      " [[ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  ...\n",
      "  [-3.1715052e-03 -3.2455730e-03 -3.3083037e-03 ... -3.0700334e-03\n",
      "   -2.9996070e-03 -2.9313334e-03]\n",
      "  [-3.1773809e-03 -3.2341788e-03 -3.3146685e-03 ... -3.0699489e-03\n",
      "   -2.9999788e-03 -2.9313846e-03]\n",
      "  [-3.1804976e-03 -3.2246064e-03 -3.3191764e-03 ... -3.0696280e-03\n",
      "   -3.0002366e-03 -2.9322300e-03]]\n",
      "\n",
      " [[ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  ...\n",
      "  [-6.7218305e-03 -5.9291925e-03 -5.3451154e-03 ... -1.8186341e-03\n",
      "   -1.7629323e-03 -1.7358430e-03]\n",
      "  [-7.0725698e-03 -6.2070461e-03 -5.5505089e-03 ... -1.7917843e-03\n",
      "   -1.7406738e-03 -1.7177539e-03]\n",
      "  [-7.4493298e-03 -6.5137306e-03 -5.7815467e-03 ... -1.7674512e-03\n",
      "   -1.7231256e-03 -1.7055334e-03]]\n",
      "\n",
      " [[ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00 -4.7191602e-04]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  1.1204694e-06\n",
      "   -1.8042247e-05 -1.5088033e-03]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  4.9782652e-06\n",
      "   -9.1096219e-05 -3.1760798e-03]\n",
      "  ...\n",
      "  [ 6.0822605e-03  6.2333788e-03  5.4031373e-03 ... -1.7911834e-03\n",
      "   -1.6541075e-03 -1.5703831e-03]\n",
      "  [ 5.9545268e-03  6.4455178e-03  5.9847669e-03 ... -1.7709677e-03\n",
      "   -1.6409208e-03 -1.5588015e-03]\n",
      "  [ 5.7220473e-03  6.5290434e-03  6.4328066e-03 ... -1.7518740e-03\n",
      "   -1.6329939e-03 -1.5520037e-03]]]\n"
     ]
    }
   ],
   "source": [
    "print(seismic_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Inspecting Training Seismic Data (seis2_1_0.npy) ====================\n",
      "\\n--- First sample, first source, first geophone, first 10 time steps ---\n",
      "[-0.00050937  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "Shape of this slice: (10,)\n",
      "\\n--- First sample, first geophone, 500th time step, for all 5 sources ---\n",
      "[-0.1284725  -0.40384248 -1.4559536   0.912251   -0.5993699 ]\n",
      "Shape of this slice: (5,)\n",
      "\\n--- Overall statistics for seismic_data_train ---\n",
      "Mean: -4.3087e-04\n",
      "Std Dev: 1.5608e+00\n",
      "Min: -2.6059e+01\n",
      "Max: 5.2216e+01\n",
      "\\n==================== Inspecting Training Velocity Data (vel2_1_0.npy) ====================\n",
      "\\n--- First sample, top-left 5x5 patch of the velocity map ---\n",
      "[[1760. 1760. 1760. 1760. 1760.]\n",
      " [1760. 1760. 1760. 1760. 1760.]\n",
      " [1760. 1760. 1760. 1760. 1760.]\n",
      " [1760. 1760. 1760. 1760. 1760.]\n",
      " [1760. 1760. 1760. 1760. 1760.]]\n",
      "Shape of this slice: (5, 5)\n",
      "\\n--- Overall statistics for velocity_data_train ---\n",
      "Mean: 3020.6394\n",
      "Std Dev: 851.3628\n",
      "Min: 1501.0000\n",
      "Max: 4500.0000\n",
      "\\n==================== Inspecting Test Seismic Data (000039dca2.npy) ====================\n",
      "Actual shape of seismic_data_test: (5, 1000, 70)\n",
      "Expanded seismic_data_test to 4D with shape: (1, 5, 1000, 70)\n",
      "\\n--- First sample, first source, first geophone, first 10 time steps (from potentially expanded 4D array) ---\n",
      "[-0.00043724  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "Shape of this slice: (10,)\n",
      "\\n--- First sample, first geophone, 500th time step, for all 5 sources (from potentially expanded 4D array) ---\n",
      "[-0.06001526  0.03909478 -0.07482627 -0.03192044  1.0524821 ]\n",
      "Shape of this slice: (5,)\n",
      "\\n--- Overall statistics for seismic_data_test (original or expanded) ---\n",
      "Mean: 1.1897e-04\n",
      "Std Dev: 1.4420e+00\n",
      "Min: -2.1378e+01\n",
      "Max: 4.0549e+01\n"
     ]
    }
   ],
   "source": [
    "# Ensure the data (seismic_data_train, velocity_data_train, seismic_data_test) \n",
    "# is loaded from the previous cell before running this.\n",
    "# If you get a NameError, re-run the cell that loads the .npy files.\n",
    "\n",
    "import numpy as np # Make sure numpy is imported in this cell or a previous one\n",
    "\n",
    "print(\"=\"*20 + \" Inspecting Training Seismic Data (seis2_1_0.npy) \" + \"=\"*20)\n",
    "if 'seismic_data_train' in locals() and hasattr(seismic_data_train, 'shape') and seismic_data_train.shape[0] > 0:\n",
    "    # First sample, first source, first geophone, first 10 time steps\n",
    "    print(\"\\\\n--- First sample, first source, first geophone, first 10 time steps ---\")\n",
    "    sample_waveform_start = seismic_data_train[0, 0, 0, :10]\n",
    "    print(sample_waveform_start)\n",
    "    print(f\"Shape of this slice: {sample_waveform_start.shape}\")\n",
    "\n",
    "    # First sample, first geophone, 500th time step, across all 5 sources\n",
    "    print(\"\\\\n--- First sample, first geophone, 500th time step, for all 5 sources ---\")\n",
    "    sample_timestep_allsources = seismic_data_train[0, :, 499, 0] # 499 is index for 500th step\n",
    "    print(sample_timestep_allsources)\n",
    "    print(f\"Shape of this slice: {sample_timestep_allsources.shape}\")\n",
    "    \n",
    "    print(\"\\\\n--- Overall statistics for seismic_data_train ---\")\n",
    "    print(f\"Mean: {np.mean(seismic_data_train):.4e}\") \n",
    "    print(f\"Std Dev: {np.std(seismic_data_train):.4e}\")\n",
    "    print(f\"Min: {np.min(seismic_data_train):.4e}\") \n",
    "    print(f\"Max: {np.max(seismic_data_train):.4e}\")\n",
    "\n",
    "else:\n",
    "    print(\"seismic_data_train not found, is not a numpy array, or is empty. Please ensure it's loaded correctly in a previous cell.\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*20 + \" Inspecting Training Velocity Data (vel2_1_0.npy) \" + \"=\"*20)\n",
    "if 'velocity_data_train' in locals() and hasattr(velocity_data_train, 'shape') and velocity_data_train.shape[0] > 0:\n",
    "    # First sample, first channel (only one), top-left 5x5 patch\n",
    "    print(\"\\\\n--- First sample, top-left 5x5 patch of the velocity map ---\")\n",
    "    sample_velocity_patch = velocity_data_train[0, 0, :5, :5]\n",
    "    print(sample_velocity_patch)\n",
    "    print(f\"Shape of this slice: {sample_velocity_patch.shape}\")\n",
    "\n",
    "    print(\"\\\\n--- Overall statistics for velocity_data_train ---\")\n",
    "    print(f\"Mean: {np.mean(velocity_data_train):.4f}\") \n",
    "    print(f\"Std Dev: {np.std(velocity_data_train):.4f}\")\n",
    "    print(f\"Min: {np.min(velocity_data_train):.4f}\")\n",
    "    print(f\"Max: {np.max(velocity_data_train):.4f}\")\n",
    "else:\n",
    "    print(\"velocity_data_train not found, is not a numpy array, or is empty. Please ensure it's loaded correctly in a previous cell.\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*20 + \" Inspecting Test Seismic Data (000039dca2.npy) \" + \"=\"*20)\n",
    "if 'seismic_data_test' in locals() and hasattr(seismic_data_test, 'shape'):\n",
    "    print(f\"Actual shape of seismic_data_test: {seismic_data_test.shape}\") # IMPORTANT: Check this output\n",
    "\n",
    "    # Standardize to 4D (add batch dimension if it's 3D) for consistent processing logic later if needed\n",
    "    # This is a common preprocessing step.\n",
    "    current_test_data = seismic_data_test\n",
    "    if seismic_data_test.ndim == 3:\n",
    "        # Assuming 3D is (C, T, G), add a batch dimension at the start\n",
    "        current_test_data = np.expand_dims(seismic_data_test, axis=0)\n",
    "        print(f\"Expanded seismic_data_test to 4D with shape: {current_test_data.shape}\")\n",
    "    \n",
    "    if current_test_data.ndim == 4 and current_test_data.shape[0] > 0 :\n",
    "        # Now current_test_data should be 4D: (S, C, T, G), where S is likely 1\n",
    "        print(\"\\\\n--- First sample, first source, first geophone, first 10 time steps (from potentially expanded 4D array) ---\")\n",
    "        # Accessing as [sample_idx, source_idx, geophone_idx, time_idx_slice]\n",
    "        # Note: The previous error indicated (C,T,G), so if original was (5,1000,70), expanded is (1,5,1000,70)\n",
    "        # Accessing 1st sample (index 0), 1st source (index 0), 1st geophone (index 0), first 10 time steps.\n",
    "        sample_test_waveform_start = current_test_data[0, 0, 0, :10] \n",
    "        print(sample_test_waveform_start)\n",
    "        print(f\"Shape of this slice: {sample_test_waveform_start.shape}\")\n",
    "\n",
    "        print(\"\\\\n--- First sample, first geophone, 500th time step, for all 5 sources (from potentially expanded 4D array) ---\")\n",
    "        sample_test_timestep_allsources = current_test_data[0, :, 499, 0] # 0th sample, all sources, 500th time, 0th geophone\n",
    "        print(sample_test_timestep_allsources)\n",
    "        print(f\"Shape of this slice: {sample_test_timestep_allsources.shape}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Test data (current_test_data) has an unexpected shape or dimension after potential expansion: {current_test_data.shape}. Cannot proceed with slicing as defined.\")\n",
    "\n",
    "    print(\"\\\\n--- Overall statistics for seismic_data_test (original or expanded) ---\")\n",
    "    print(f\"Mean: {np.mean(current_test_data):.4e}\")\n",
    "    print(f\"Std Dev: {np.std(current_test_data):.4e}\")\n",
    "    print(f\"Min: {np.min(current_test_data):.4e}\")\n",
    "    print(f\"Max: {np.max(current_test_data):.4e}\")\n",
    "else:\n",
    "    print(\"seismic_data_test not found or is not a numpy array. Please ensure it's loaded correctly in a previous cell.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Yale_UNC_FWI)",
   "language": "python",
   "name": "yale_unc_fwi_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
